{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82097412",
   "metadata": {},
   "source": [
    "# TrasnformerのEncoderの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5b0b9",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1dcc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f8a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数の固定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568dcf0",
   "metadata": {},
   "source": [
    "## 2. モデルの実装と動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f78560",
   "metadata": {},
   "source": [
    "### 2-1. 単語埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e00037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    '''\n",
    "    埋め込みベクトルの計算を行う\n",
    "    idで示されている単語をベクトルに変換\n",
    "    '''\n",
    "\n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "            embeddings=text_embedding_vectors, freeze=True)\n",
    "        # freeze=Trueに設定すると、逆伝播の際に更新されなくなる\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        x_vec = self.embeddings(x)\n",
    "\n",
    "        return x_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5ca6e",
   "metadata": {},
   "source": [
    "### 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772db763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 999994/999994 [01:27<00:00, 11375.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([24, 256])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderなどを取得\n",
    "from utils.dataloader import get_IMDb_DataLoaders_and_TEXT\n",
    "\n",
    "# ./data/aclImdb_v1.tar.gzがない場合は、ダウンロードに時間がかかる\n",
    "# ./data/wiki-news-300d-1M.vec.zipがない場合は、ダウンロードに時間がかかる\n",
    "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(max_length=256, batch_size=24)\n",
    "\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x.shape)\n",
    "print(\"出力のテンソルサイズ：\", x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7664247",
   "metadata": {},
   "source": [
    "### 2-2. 位置エンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7961ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    '''\n",
    "    位置エンコーディングの計算\n",
    "    入力された単語の位置を示す情報をベクトルに加える\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # 埋め込みベクトルの次元数\n",
    "        self.d_model = d_model  \n",
    "\n",
    "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # GPUが使える場合はGPUへ送る（ここでは省略、学習時には使用する）\n",
    "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # pe = pe.to(device)\n",
    "\n",
    "        # 位置エンコーディングの値を計算\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2*i)/d_model)))\n",
    "\n",
    "        # peの先頭に、ミニバッチを表す次元を追加\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        # 入力xとpeを足し算する\n",
    "        # xがpeよりも小さいので、次元数の平方根を掛けて大きくする\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc96e1",
   "metadata": {},
   "source": [
    "### 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35d5844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "x2 = net2(x1)\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x1.shape)\n",
    "print(\"出力のテンソルサイズ：\", x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae37e50",
   "metadata": {},
   "source": [
    "### 2-3. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba11f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    本来のTransformerはマルチヘッドAttentionだが\n",
    "    分かりやすさを優先してシングルAttentionで実装\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層を用いて特徴量をQ,K,Vに変換する\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 出力時に使用する全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        \n",
    "        # 全結合層で特徴量を変換\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxを用いて、Attentionの値を0〜1の範囲に収める\n",
    "        normlized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(normlized_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, normlized_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634fa19",
   "metadata": {},
   "source": [
    "### 2-4. FeedForwardブロック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0678bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        '''\n",
    "        FeedForwardブロック\n",
    "        Attention層からの出力を、全結合層2つで変換する\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a126d",
   "metadata": {},
   "source": [
    "### 2-5. Transformerブロック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b2845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    Transformerブロック\n",
    "    レイヤー正規化・Attention・FeedFoward・Dropoutで構成される\n",
    "    '''\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # レイヤー正規化\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "\n",
    "        # Attentionのあとの全結合層2つ\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        '''\n",
    "        順伝播\n",
    "        '''\n",
    "        \n",
    "        # レイヤー正規化\n",
    "        x_normlized = self.norm_1(x)\n",
    "\n",
    "        # Attentionの計算\n",
    "        output, normlized_weights = self.attn(\n",
    "            x_normlized, x_normlized, x_normlized, mask)\n",
    "        \n",
    "        # Dropoutを通したものを加える\n",
    "        x2 = x + self.dropout_1(output)\n",
    "\n",
    "        # レイヤー正規化\n",
    "        x_normlized2 = self.norm_2(x2)\n",
    "        \n",
    "        # 全結合層とDropoutを通したものを加える\n",
    "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
    "\n",
    "        return output, normlized_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58637d01",
   "metadata": {},
   "source": [
    "### 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33624b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])\n",
      "入力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "Attentionのサイズ： torch.Size([24, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "# maskの作成\n",
    "x = batch.Text[0]\n",
    "input_pad = 1  # 単語IDにおいて、'<pad>': 1 であるため\n",
    "input_mask = (x != input_pad)\n",
    "print(input_mask[0])\n",
    "\n",
    "# 入出力\n",
    "x1 = net1(x)  # 単語をベクトルに変換\n",
    "x2 = net2(x1)  # 位置エンコーディングを加算\n",
    "x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x2.shape)\n",
    "print(\"出力のテンソルサイズ：\", x3.shape)\n",
    "print(\"Attentionのサイズ：\", normlized_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26bb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
